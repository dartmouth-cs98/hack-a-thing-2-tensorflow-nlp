{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "with open('./trump_tweets.json') as f:\n",
    "    data = json.load(f)\n",
    "documents = []\n",
    "for tweet in data:\n",
    "    # remove retweets\n",
    "    if tweet['text'][:2] != \"RT\":\n",
    "        documents.append(tweet['text'])\n"
   ]
  },
  {
   "source": [
    "glancing at the data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "44729\n['Will be doing show with @RushLimbaughEIB at 12:00 P.M. TALK RADIO.  ENJOY!!!', 'Covid Relief Negotiations are moving along. Go Big!', 'Just got a briefing on Hurricane Delta rushing toward Louisiana and Mississippi. @fema is there and ready!!!', 'Crazy Nancy Pelosi is looking at the 25th Amendment in order to replace Joe Biden with Kamala Harris. The Dems want that to happen fast because Sleepy Joe is out of it!!!', 'Steve Scully, the second Debate Moderator, is a Never Trumper, just like the son of the great Mike Wallace. Fix!!!']\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/erafkin/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import string\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "source": [
    "A function to perform lemmatize and stem preprocessing steps on the data set:\n",
    "- no @'s\n",
    "- no links"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    temp = []\n",
    "    for token in text.split(\" \"):\n",
    "        token = token.lower()\n",
    "        if token[:1] != '@' and token[:4] != 'http':\n",
    "            token = token.translate(str.maketrans('', '', string.punctuation))\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "source": [
    "picking a doc to preview after preprocessing:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original document: \n",
      "['Did', 'anyone', 'get', 'to', 'see', 'that', 'absolute', '“Joke”', 'of', 'a', 'Town', 'Hall', 'interview', 'that', 'Joe', 'Biden', 'did', 'with', 'Concast', '@NBCNews', ',', 'hosted', 'by', 'Lester', 'Holt?', 'What', 'a', 'disgrace', 'to', 'our', 'Country', 'that', 'FREE', 'public', 'airwaves', 'can', 'be', 'used', 'that', 'way.', 'All', 'SOFTBALLS.', 'A', 'big', 'FIX.', 'Time', 'should', 'be', 'paid', 'by', 'the', 'corrupt', 'DNC!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['absolut', '“joke”', 'town', 'hall', 'interview', 'biden', 'concast', 'host', 'lester', 'holt', 'disgrac', 'countri', 'free', 'public', 'airwav', 'softbal', 'time', 'pay', 'corrupt']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[100]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "source": [
    "Preprocess the headline text, saving the results as ‘processed_docs’"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['1200', 'talk', 'radio', 'enjoy'],\n",
       " ['covid', 'relief', 'negoti', 'move'],\n",
       " ['brief', 'hurrican', 'delta', 'rush', 'louisiana', 'mississippi', 'readi'],\n",
       " ['crazi',\n",
       "  'nanci',\n",
       "  'pelosi',\n",
       "  'look',\n",
       "  '25th',\n",
       "  'amend',\n",
       "  'order',\n",
       "  'replac',\n",
       "  'biden',\n",
       "  'kamala',\n",
       "  'harri',\n",
       "  'dem',\n",
       "  'want',\n",
       "  'happen',\n",
       "  'fast',\n",
       "  'sleepi'],\n",
       " ['steve',\n",
       "  'sculli',\n",
       "  'second',\n",
       "  'debat',\n",
       "  'moder',\n",
       "  'trumper',\n",
       "  'like',\n",
       "  'great',\n",
       "  'mike',\n",
       "  'wallac'],\n",
       " ['vote'],\n",
       " ['hello'],\n",
       " ['save', 'second', 'amend', 'virginia', 'go', 'away', 'vote', 'trump'],\n",
       " ['nick',\n",
       "  'complet',\n",
       "  'total',\n",
       "  'endors',\n",
       "  'warrior',\n",
       "  'virginia',\n",
       "  'protect',\n",
       "  'second',\n",
       "  'amend'],\n",
       " ['gallup',\n",
       "  'poll',\n",
       "  'come',\n",
       "  'incred',\n",
       "  'find',\n",
       "  'better',\n",
       "  'today',\n",
       "  'pandem',\n",
       "  'year',\n",
       "  'obiden',\n",
       "  'highest',\n",
       "  'number',\n",
       "  'record',\n",
       "  'pretti',\n",
       "  'amaz']]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "processed_docs = []\n",
    "for doc in documents:\n",
    "    processed_docs.append(preprocess(doc))\n",
    "processed_docs[:10]"
   ]
  },
  {
   "source": [
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 1200\n1 enjoy\n2 radio\n3 talk\n4 covid\n5 move\n6 negoti\n7 relief\n8 brief\n9 delta\n10 hurrican\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "source": [
    "\"Filter out everything that's dumb\"\n",
    "- tokens that appear in less than 15 docs\n",
    "- tokens that appear in more half of the docs\n",
    "- keep only the first 100000 most frequent tokens\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=20000)"
   ]
  },
  {
   "source": [
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(14, 1),\n",
       " (101, 1),\n",
       " (111, 1),\n",
       " (146, 1),\n",
       " (262, 1),\n",
       " (266, 1),\n",
       " (276, 1),\n",
       " (367, 1),\n",
       " (376, 1),\n",
       " (390, 1),\n",
       " (436, 1),\n",
       " (437, 1),\n",
       " (438, 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[100]"
   ]
  },
  {
   "source": [
    "preview our bag o' words:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word 14 (\"biden\") appears 1 time.\nWord 101 (\"countri\") appears 1 time.\nWord 111 (\"interview\") appears 1 time.\nWord 146 (\"absolut\") appears 1 time.\nWord 262 (\"hall\") appears 1 time.\nWord 266 (\"town\") appears 1 time.\nWord 276 (\"corrupt\") appears 1 time.\nWord 367 (\"pay\") appears 1 time.\nWord 376 (\"disgrac\") appears 1 time.\nWord 390 (\"time\") appears 1 time.\nWord 436 (\"free\") appears 1 time.\nWord 437 (\"host\") appears 1 time.\nWord 438 (\"public\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[100]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "source": [
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document.\n",
    "tf-idf stands for \"term frequency inverse document frequency\". It is an idicator of the importance of a word. A high score means the word is important, and a low score means less important. It's based on the idea that a word that is very frequent in an individual doc and is very frequent over the entire corpus of documents is unimportant (ex. the, of). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 0.4669044380529935), (1, 0.7649100709039481), (2, 0.4437486103145963)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "source": [
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 \nWords: 0.023*\"great\" + 0.017*\"deal\" + 0.015*\"obama\" + 0.014*\"america\" + 0.014*\"china\" + 0.009*\"vote\" + 0.009*\"obamacar\" + 0.007*\"iran\" + 0.007*\"spend\" + 0.006*\"need\"\nTopic: 1 \nWords: 0.023*\"cont\" + 0.012*\"like\" + 0.011*\"year\" + 0.010*\"great\" + 0.008*\"trump\" + 0.007*\"go\" + 0.007*\"debt\" + 0.007*\"success\" + 0.007*\"happi\" + 0.007*\"peopl\"\nTopic: 2 \nWords: 0.031*\"great\" + 0.025*\"dont\" + 0.012*\"thank\" + 0.012*\"good\" + 0.012*\"today\" + 0.010*\"hotel\" + 0.010*\"beauti\" + 0.010*\"work\" + 0.010*\"miss\" + 0.009*\"look\"\nTopic: 3 \nWords: 0.060*\"trump\" + 0.030*\"donald\" + 0.020*\"watch\" + 0.020*\"interview\" + 0.017*\"apprentic\" + 0.017*\"tonight\" + 0.014*\"great\" + 0.013*\"night\" + 0.010*\"celebr\" + 0.009*\"thank\"\nTopic: 4 \nWords: 0.053*\"thank\" + 0.022*\"presid\" + 0.017*\"great\" + 0.015*\"trump\" + 0.015*\"countri\" + 0.013*\"need\" + 0.010*\"america\" + 0.010*\"peopl\" + 0.010*\"want\" + 0.010*\"true\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "source": [
    "running LDA using TF-IDF:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 Word: 0.023*\"great\" + 0.011*\"good\" + 0.010*\"america\" + 0.010*\"trump\" + 0.009*\"thank\" + 0.007*\"golf\" + 0.007*\"work\" + 0.006*\"agre\" + 0.006*\"luck\" + 0.006*\"enjoy\"\nTopic: 1 Word: 0.014*\"presid\" + 0.012*\"trump\" + 0.009*\"dont\" + 0.008*\"tonight\" + 0.007*\"interview\" + 0.007*\"great\" + 0.007*\"makeamericagreatagain\" + 0.007*\"watch\" + 0.006*\"trump2016\" + 0.006*\"thank\"\nTopic: 2 Word: 0.076*\"thank\" + 0.012*\"great\" + 0.010*\"happi\" + 0.007*\"vote\" + 0.006*\"birthday\" + 0.006*\"support\" + 0.005*\"trump\" + 0.005*\"peopl\" + 0.005*\"love\" + 0.004*\"congratul\"\nTopic: 3 Word: 0.010*\"china\" + 0.007*\"obama\" + 0.006*\"time\" + 0.006*\"countri\" + 0.006*\"trump\" + 0.005*\"great\" + 0.004*\"presid\" + 0.004*\"peopl\" + 0.004*\"stop\" + 0.004*\"think\"\nTopic: 4 Word: 0.016*\"true\" + 0.012*\"trump\" + 0.010*\"donald\" + 0.009*\"cont\" + 0.007*\"love\" + 0.007*\"apprentic\" + 0.007*\"2016\" + 0.007*\"great\" + 0.007*\"presid\" + 0.006*\"obama\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "source": [
    "Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "\n",
    "We will check where our test document would be classified."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['stock',\n",
       " 'market',\n",
       " 'point',\n",
       " '28149',\n",
       " 'great',\n",
       " 'news',\n",
       " 'america',\n",
       " 'job',\n",
       " 'job',\n",
       " 'job']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "processed_docs[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nScore: 0.7242588400840759\t \nTopic: 0.053*\"thank\" + 0.022*\"presid\" + 0.017*\"great\" + 0.015*\"trump\" + 0.015*\"countri\" + 0.013*\"need\" + 0.010*\"america\" + 0.010*\"peopl\" + 0.010*\"want\" + 0.010*\"true\"\n\nScore: 0.2147570252418518\t \nTopic: 0.023*\"cont\" + 0.012*\"like\" + 0.011*\"year\" + 0.010*\"great\" + 0.008*\"trump\" + 0.007*\"go\" + 0.007*\"debt\" + 0.007*\"success\" + 0.007*\"happi\" + 0.007*\"peopl\"\n\nScore: 0.020439056679606438\t \nTopic: 0.023*\"great\" + 0.017*\"deal\" + 0.015*\"obama\" + 0.014*\"america\" + 0.014*\"china\" + 0.009*\"vote\" + 0.009*\"obamacar\" + 0.007*\"iran\" + 0.007*\"spend\" + 0.006*\"need\"\n\nScore: 0.020310547202825546\t \nTopic: 0.060*\"trump\" + 0.030*\"donald\" + 0.020*\"watch\" + 0.020*\"interview\" + 0.017*\"apprentic\" + 0.017*\"tonight\" + 0.014*\"great\" + 0.013*\"night\" + 0.010*\"celebr\" + 0.009*\"thank\"\n\nScore: 0.02023455873131752\t \nTopic: 0.031*\"great\" + 0.025*\"dont\" + 0.012*\"thank\" + 0.012*\"good\" + 0.012*\"today\" + 0.010*\"hotel\" + 0.010*\"beauti\" + 0.010*\"work\" + 0.010*\"miss\" + 0.009*\"look\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[120]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "source": [
    "try one more:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['half',\n",
       " 'year',\n",
       " 'secur',\n",
       " 'america',\n",
       " 'border',\n",
       " 'rebuild',\n",
       " 'awesom',\n",
       " 'power',\n",
       " 'militari',\n",
       " 'obliter',\n",
       " 'isi',\n",
       " 'caliph',\n",
       " 'fix',\n",
       " 'disastr',\n",
       " 'trade',\n",
       " 'deal',\n",
       " 'bring',\n",
       " 'job',\n",
       " 'home',\n",
       " 'america',\n",
       " 'minnesota',\n",
       " 'maga']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "processed_docs[170]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nScore: 0.922659695148468\t \nTopic: 0.023*\"great\" + 0.017*\"deal\" + 0.015*\"obama\" + 0.014*\"america\" + 0.014*\"china\" + 0.009*\"vote\" + 0.009*\"obamacar\" + 0.007*\"iran\" + 0.007*\"spend\" + 0.006*\"need\"\n\nScore: 0.04938477277755737\t \nTopic: 0.060*\"trump\" + 0.030*\"donald\" + 0.020*\"watch\" + 0.020*\"interview\" + 0.017*\"apprentic\" + 0.017*\"tonight\" + 0.014*\"great\" + 0.013*\"night\" + 0.010*\"celebr\" + 0.009*\"thank\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[170]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "source": [
    "Performance evaluation by classifying sample document using LDA TF-IDF model:\n",
    "- we like tf-idf because we think it is classifiying better"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nScore: 0.6920978426933289\t \nTopic: 0.010*\"china\" + 0.007*\"obama\" + 0.006*\"time\" + 0.006*\"countri\" + 0.006*\"trump\" + 0.005*\"great\" + 0.004*\"presid\" + 0.004*\"peopl\" + 0.004*\"stop\" + 0.004*\"think\"\n\nScore: 0.19135954976081848\t \nTopic: 0.014*\"presid\" + 0.012*\"trump\" + 0.009*\"dont\" + 0.008*\"tonight\" + 0.007*\"interview\" + 0.007*\"great\" + 0.007*\"makeamericagreatagain\" + 0.007*\"watch\" + 0.006*\"trump2016\" + 0.006*\"thank\"\n\nScore: 0.03958211466670036\t \nTopic: 0.023*\"great\" + 0.011*\"good\" + 0.010*\"america\" + 0.010*\"trump\" + 0.009*\"thank\" + 0.007*\"golf\" + 0.007*\"work\" + 0.006*\"agre\" + 0.006*\"luck\" + 0.006*\"enjoy\"\n\nScore: 0.038685038685798645\t \nTopic: 0.076*\"thank\" + 0.012*\"great\" + 0.010*\"happi\" + 0.007*\"vote\" + 0.006*\"birthday\" + 0.006*\"support\" + 0.005*\"trump\" + 0.005*\"peopl\" + 0.005*\"love\" + 0.004*\"congratul\"\n\nScore: 0.03827548027038574\t \nTopic: 0.016*\"true\" + 0.012*\"trump\" + 0.010*\"donald\" + 0.009*\"cont\" + 0.007*\"love\" + 0.007*\"apprentic\" + 0.007*\"2016\" + 0.007*\"great\" + 0.007*\"presid\" + 0.006*\"obama\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[corpus_tfidf[170]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "source": [
    "Real deal: testing on unseen document"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 0.8375043272972107\t Topic: 0.023*\"great\" + 0.017*\"deal\" + 0.015*\"obama\" + 0.014*\"america\" + 0.014*\"china\"\nScore: 0.0410543717443943\t Topic: 0.023*\"cont\" + 0.012*\"like\" + 0.011*\"year\" + 0.010*\"great\" + 0.008*\"trump\"\nScore: 0.04051666334271431\t Topic: 0.053*\"thank\" + 0.022*\"presid\" + 0.017*\"great\" + 0.015*\"trump\" + 0.015*\"countri\"\nScore: 0.04047143831849098\t Topic: 0.031*\"great\" + 0.025*\"dont\" + 0.012*\"thank\" + 0.012*\"good\" + 0.012*\"today\"\nScore: 0.0404532290995121\t Topic: 0.060*\"trump\" + 0.030*\"donald\" + 0.020*\"watch\" + 0.020*\"interview\" + 0.017*\"apprentic\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'if i win the election the rich won\\'t have to pay taxes'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "source": [
    "### Conclusion\n",
    "\n",
    "this one makes a little more sense. but the parser needs to be better. a lot of words that don't matter were left over. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}